{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only topic model - inv - exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "parser = {}\n",
    "parser['gpu'] = '0'\n",
    "parser['seed']=int(time.time())#1234\n",
    "parser['log_period']=200#625#3000\n",
    "parser['max_to_keep']=10\n",
    "parser['n_freq'] = 10\n",
    "parser['n_epochs']=410#1000\n",
    "parser['batch_size']=512\n",
    "parser['opt'] = 'Adagrad'\n",
    "parser['lr']=0.01#0.005\n",
    "parser['reg']=0.1\n",
    "parser['reg3']=1000.\n",
    "parser['reg4']=1.\n",
    "\n",
    "parser['keep_prob']=0.8\n",
    "parser['dim_hidden_bow']=100#256\n",
    "parser['dim_latent_bow']=2#2#100\n",
    "parser['dim_latent_path']=2#100\n",
    "\n",
    "parser['dim_emb']=256#256\n",
    "parser['tree']=63#121#126#63#126\n",
    "parser['n_depth']=4\n",
    "parser['depth_temperature']=0.1#10\n",
    "#parser['add_threshold']={1: 0.065, 2:0.04} #> add\n",
    "\n",
    "# min_child = 3 # add_threshold (1/min_child) ** level, max_level(r) > add_threshold\n",
    "# max_child = 5\n",
    "# max_depth = 3\n",
    "\n",
    "#parser['remove_threshold']=0.01 #< remove, #(1/max_child) ** level\n",
    "parser['cell']='rnn'\n",
    "parser['static']=False\n",
    "# parser['path_data']='data/jacm/instances.pkl'\n",
    "# parser['dir_model']='model/jacm/checkpoint_tmp'\n",
    "\n",
    "parser['data_type'] ='bbc'\n",
    "parser['dist_type']='inv'\n",
    "parser['path_data']='data/'+parser['data_type']+'/instances.pkl'\n",
    "# parser['path_data']='data/20news/instances.pkl'\n",
    "parser['dir_model']='model/'+parser['data_type']+'/checkpoint_tmp'\n",
    "# parser['train_labels']='data/20news_with_labels/train_20news_labels.npy'\n",
    "parser['tmp']=False\n",
    "\n",
    "doc_len = {'bbc': 2225, 'reuters':7674, '20news':18181, 'mendeley': 46985}\n",
    "parser['tree_expected_min_child'] = 5# math.ceil(math.ceil(np.cbrt(doc_len[parser['data_type']]/2.))/3.)#3#6#3#6\n",
    "parser['tree_expected_max_child'] = 10#math.ceil(np.cbrt(doc_len[parser['data_type']]/2.))#6#15#6#15\n",
    "parser['tree_max_depth'] = 4\n",
    "\n",
    "parser['vis']=True\n",
    "\n",
    "config = SimpleNamespace(**parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [1, 2, 3], 1: [21, 22, 23], 2: [41, 42, 43], 3: [61, 62, 63]}\n",
      "[0, 1, 2, 3, 21, 22, 23, 41, 42, 43, 61, 62, 63]\n",
      "self.child_to_parent_idxs\n",
      "{1: 0, 2: 0, 3: 0, 21: 1, 22: 1, 23: 1, 41: 2, 42: 2, 43: 2, 61: 3, 62: 3, 63: 3}\n",
      "self.tree_depth:  {0: 1, 1: 2, 21: 3, 22: 3, 23: 3, 2: 2, 41: 3, 42: 3, 43: 3, 3: 2, 61: 3, 62: 3, 63: 3}\n",
      "self.n_depth:  3\n",
      "self.level_nodes:  {0: [0], 1: [1, 2, 3], 2: [21, 22, 23, 41, 42, 43, 61, 62, 63]}\n",
      "child_to_parent_idxs get_leaf_parents {1: 0, 2: 0, 3: 0, 21: 1, 22: 1, 23: 1, 41: 2, 42: 2, 43: 2, 61: 3, 62: 3, 63: 3}\n",
      "tree_idxs get_leaf_parents {0: [1, 2, 3], 1: [21, 22, 23], 2: [41, 42, 43], 3: [61, 62, 63]}\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:552: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:554: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:556: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:561: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:566: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:569: The name tf.layers.Dropout is deprecated. Please use tf.compat.v1.layers.Dropout instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:572: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:595: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "leaf_ancestor_idxs:  {21: [0, 1, 21], 22: [0, 1, 22], 23: [0, 1, 23], 41: [0, 2, 41], 42: [0, 2, 42], 43: [0, 2, 43], 61: [0, 3, 61], 62: [0, 3, 62], 63: [0, 3, 63]}\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:148: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:449: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n",
      "self.config.reg3:  1000.0\n",
      "WARNING:tensorflow:From E:\\source_code\\htv\\model.py:657: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From E:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:281: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "WARNING:tensorflow:From E:\\anaconda\\envs\\py36\\lib\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "---START TRAINING----------\n",
      "epoch:  200\n",
      "[[449574.75, 859.67175, 3.2375274, 14.001608], [458244.7, 876.4599, 3.3828769, 14.001608], [516992.7, 991.48975, 3.0951996, 14.001608], [500338.44, 958.20844, 3.8486652, 14.001608]]\n",
      "KNN:  [0.91162109375, 0.9033203125, 0.90283203125, 0.90087890625, 0.900390625]\n",
      "\n",
      "        Time            47\n",
      "        Ep             200\n",
      "        Ct               4\n",
      "TRAIN:  LOSS     483011.06\n",
      "        PPL           1014\n",
      "        NLL         924.81\n",
      "        KL            3.35\n",
      "        REG          14.37\n",
      "VALID:  LOSS     481287.62\n",
      "        PPL            993\n",
      "        NLL         921.46\n",
      "        KL            3.39\n",
      "        REG          14.00\n",
      "SPEC:   1             0.19\n",
      "        2             0.38\n",
      "        3             0.70\n",
      "HIER:   CHILD         0.37\n",
      "        OTHER         0.06\n",
      "dtype: object\n",
      "0 R: 1.000 P: 0.267 will year time work told parti well week call set\n",
      "   1 R: 0.283 P: 0.135 govern year compani firm plan countri sale month expect will\n",
      "     21 R: 0.050 P: 0.050 technolog mobil phone user comput digit system softwar site net\n",
      "     22 R: 0.047 P: 0.047 share growth rise bank econom price profit cut analyst budget\n",
      "     23 R: 0.051 P: 0.051 tax market firm economi compani increas rate cut busi eu\n",
      "\n",
      "   2 R: 0.219 P: 0.106 play game good second start win time open england player\n",
      "     41 R: 0.034 P: 0.034 labour brown elect chancellor blair year gordon tori deliv toni\n",
      "     42 R: 0.039 P: 0.039 player win game england side team play club ireland final\n",
      "     43 R: 0.040 P: 0.040 match olymp win cup coach player injuri leagu goal season\n",
      "\n",
      "   3 R: 0.231 P: 0.111 will best game music includ bbc number peopl top record\n",
      "     61 R: 0.038 P: 0.038 film won star music game award play box athlet winner\n",
      "     62 R: 0.037 P: 0.037 award star actor film song band prize nomin album best\n",
      "     63 R: 0.044 P: 0.044 peopl servic network video onlin download tv medium popular creat\n",
      "\n",
      "\n",
      "{0: [1, 2, 3, 4], 1: [21, 22, 23, 24], 2: [41, 42, 43, 44], 3: [61, 62, 63, 64], 23: [461, 462]}\n",
      "[0, 1, 2, 3, 4, 21, 22, 23, 24, 41, 42, 43, 44, 61, 62, 63, 64, 461, 462]\n",
      "self.child_to_parent_idxs\n",
      "{1: 0, 2: 0, 3: 0, 4: 0, 21: 1, 22: 1, 23: 1, 24: 1, 41: 2, 42: 2, 43: 2, 44: 2, 61: 3, 62: 3, 63: 3, 64: 3, 461: 23, 462: 23}\n",
      "self.tree_depth:  {0: 1, 1: 2, 21: 3, 22: 3, 23: 3, 461: 4, 462: 4, 24: 3, 2: 2, 41: 3, 42: 3, 43: 3, 44: 3, 3: 2, 61: 3, 62: 3, 63: 3, 64: 3, 4: 2}\n",
      "self.n_depth:  4\n",
      "self.level_nodes:  {0: [0], 1: [1, 2, 3, 4], 2: [21, 22, 23, 24, 41, 42, 43, 44, 61, 62, 63, 64], 3: [461, 462]}\n",
      "child_to_parent_idxs get_leaf_parents {1: 0, 2: 0, 3: 0, 4: 0, 21: 1, 22: 1, 23: 1, 24: 1, 41: 2, 42: 2, 43: 2, 44: 2, 61: 3, 62: 3, 63: 3, 64: 3, 461: 23, 462: 23}\n",
      "tree_idxs get_leaf_parents {0: [1, 2, 3, 4], 1: [21, 22, 23, 24], 2: [41, 42, 43, 44], 3: [61, 62, 63, 64], 23: [461, 462]}\n",
      "leaf_ancestor_idxs:  {4: [0, 4], 21: [0, 1, 21], 22: [0, 1, 22], 24: [0, 1, 24], 41: [0, 2, 41], 42: [0, 2, 42], 43: [0, 2, 43], 44: [0, 2, 44], 61: [0, 3, 61], 62: [0, 3, 62], 63: [0, 3, 63], 64: [0, 3, 64], 461: [0, 1, 23, 461], 462: [0, 1, 23, 462]}\n",
      "self.config.reg3:  1000.0\n",
      "epoch:  400\n",
      "[[444518.75, 849.73303, 4.195633, 40.504433], [450463.03, 861.3766, 4.1620035, 40.504433], [509332.9, 976.6605, 3.858279, 40.504433], [497287.62, 952.2113, 4.781538, 40.504433]]\n",
      "KNN:  [0.94970703125, 0.94140625, 0.94140625, 0.939453125, 0.93896484375]\n",
      "\n",
      "        Time            53\n",
      "        Ep             400\n",
      "        Ct               4\n",
      "TRAIN:  LOSS     476307.47\n",
      "        PPL            932\n",
      "        NLL         911.74\n",
      "        KL            4.28\n",
      "        REG          40.63\n",
      "VALID:  LOSS     475400.56\n",
      "        PPL            922\n",
      "        NLL         910.00\n",
      "        KL            4.25\n",
      "        REG          40.50\n",
      "SPEC:   1             0.19\n",
      "        2             0.47\n",
      "        3             0.75\n",
      "HIER:   CHILD         0.34\n",
      "        OTHER         0.07\n",
      "dtype: object\n",
      "0 R: 1.000 P: 0.210 will year peopl work time told govern plan call public\n",
      "   1 R: 0.242 P: 0.088 compani year firm sale market month report cost busi oper\n",
      "     21 R: 0.039 P: 0.039 technolog mobil phone user comput net site softwar data system\n",
      "     22 R: 0.036 P: 0.036 tax cut economi increas econom spend countri budget govern rise\n",
      "     23 R: 0.039 P: 0.025 year govern countri job expect european meet pay report plan\n",
      "       461 R: 0.008 P: 0.008 rate eu increas growth rise tax econom economi economist cut\n",
      "       462 R: 0.006 P: 0.006 union save argentina pace pressur level lift nation brazil poverti\n",
      "\n",
      "     24 R: 0.041 P: 0.041 bank price share analyst profit oil market china euro firm\n",
      "\n",
      "   2 R: 0.190 P: 0.077 game play win england second final open start good time\n",
      "     41 R: 0.030 P: 0.030 labour elect blair tori brown prime chancellor conserv claim howard\n",
      "     42 R: 0.030 P: 0.030 player game play club win side wale william team footbal\n",
      "     43 R: 0.032 P: 0.032 match cup player coach goal team leagu injuri chelsea arsen\n",
      "     44 R: 0.021 P: 0.021 olymp race champion athlet rugbi championship titl finish athen won\n",
      "\n",
      "   3 R: 0.174 P: 0.071 music includ game uk top number releas play tv record\n",
      "     61 R: 0.024 P: 0.024 man fan event stage game seri young book contest host\n",
      "     62 R: 0.023 P: 0.023 star band film festiv singer love celebr album music rock\n",
      "     63 R: 0.034 P: 0.034 peopl servic digit video onlin websit download network tv content\n",
      "     64 R: 0.022 P: 0.022 film best award won actor nomin song oscar prize actress\n",
      "\n",
      "   4 R: 0.184 P: 0.184 will year peopl parti well time lord britain told power\n",
      "\n",
      "{0: [1, 2, 3, 4, 5], 1: [21, 22, 23, 24], 2: [41, 42, 43], 3: [63], 4: [81, 82]}\n",
      "[0, 1, 2, 3, 4, 5, 21, 22, 23, 24, 41, 42, 43, 63, 81, 82]\n",
      "self.child_to_parent_idxs\n",
      "{1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 21: 1, 22: 1, 23: 1, 24: 1, 41: 2, 42: 2, 43: 2, 63: 3, 81: 4, 82: 4}\n",
      "self.tree_depth:  {0: 1, 1: 2, 21: 3, 22: 3, 23: 3, 24: 3, 2: 2, 41: 3, 42: 3, 43: 3, 3: 2, 63: 3, 4: 2, 81: 3, 82: 3, 5: 2}\n",
      "self.n_depth:  3\n",
      "self.level_nodes:  {0: [0], 1: [1, 2, 3, 4, 5], 2: [21, 22, 23, 24, 41, 42, 43, 63, 81, 82]}\n",
      "child_to_parent_idxs get_leaf_parents {1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 21: 1, 22: 1, 23: 1, 24: 1, 41: 2, 42: 2, 43: 2, 63: 3, 81: 4, 82: 4}\n",
      "tree_idxs get_leaf_parents {0: [1, 2, 3, 4, 5], 1: [21, 22, 23, 24], 2: [41, 42, 43], 3: [63], 4: [81, 82]}\n",
      "leaf_ancestor_idxs:  {5: [0, 5], 21: [0, 1, 21], 22: [0, 1, 22], 23: [0, 1, 23], 24: [0, 1, 24], 41: [0, 2, 41], 42: [0, 2, 42], 43: [0, 2, 43], 63: [0, 3, 63], 81: [0, 4, 81], 82: [0, 4, 82]}\n",
      "self.config.reg3:  1000.0\n"
     ]
    }
   ],
   "source": [
    "for n_time in range(1):\n",
    "    n_time=3\n",
    "    config.vis=True\n",
    "    config.tree_idxs = get_tree_idxs(config.tree)\n",
    "    config.path_model = os.path.join(config.dir_model, 'model') \n",
    "    config.path_config = config.path_model + '-%i.config'\n",
    "    config.path_log = os.path.join(config.dir_model, 'log')\n",
    "    config.path_checkpoint = os.path.join(config.dir_model, 'checkpoint')\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.gpu\n",
    "    np.random.seed(config.seed)\n",
    "    random.seed(config.seed)\n",
    "    # load data\n",
    "    instances_train, _, _, word_to_idx, idx_to_word, bow_idxs = cPickle.load(open(config.path_data,'rb'))\n",
    "    train_batches = get_batches(instances_train, config.batch_size)\n",
    "    #dev_batches = get_batches(instances_dev, config.batch_size)\n",
    "    config.dim_bow = len(bow_idxs)\n",
    "\n",
    "    # initialize log\n",
    "    checkpoint = []\n",
    "    losses_train = []\n",
    "    ppls_train = []\n",
    "    ppl_min = np.inf\n",
    "    epoch = 0\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "\n",
    "    log_df = pd.DataFrame(columns=pd.MultiIndex.from_tuples(\n",
    "                        list(zip(*[['','','','TRAIN:','','','','','VALID:','','','','', 'SPEC:', '', '', 'HIER:', ''],\n",
    "                                ['Time','Ep','Ct','LOSS','PPL','NLL','KL','REG','LOSS','PPL','NLL','KL','REG', '1', '2', '3', 'CHILD', 'OTHER']]))))\n",
    "\n",
    "\n",
    "    # initialize model\n",
    "    if 'sess' in globals(): sess.close()\n",
    "    model = HierarchicalNeuralTopicModel(config)\n",
    "    # sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "    sess = tf.InteractiveSession(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #========================== FR ====================\n",
    "    v = model.topic_idxs\n",
    "    topic_coords = sess.run(model.topic_coords)\n",
    "    tree = model.tree_idxs\n",
    "    pairs = [[(parent,j) for j in childs] for parent,childs in tree.items()]\n",
    "    flat_pairs = [item for sublist in pairs for item in sublist]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(v)\n",
    "    G.add_edges_from(flat_pairs)\n",
    "    # initial position\n",
    "    for v in nx.nodes(G):\n",
    "        G.nodes[v]['x'] = topic_coords[v][0][0]\n",
    "        G.nodes[v]['y'] = topic_coords[v][0][1]\n",
    "    pos = {}\n",
    "    for v in G.nodes():\n",
    "        pos[v] = [G.nodes[v]['x'],G.nodes[v]['y']]\n",
    "\n",
    "#     pos=nx.fruchterman_reingold_layout(G, scale=0.1)    \n",
    "    pos=nx.kamada_kawai_layout(G, scale=0.1)    \n",
    "\n",
    "    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "    for name in name_tensors.keys():\n",
    "        if ('Adagrad' not in name) and ('topic_coords' in name):\n",
    "            node_idx = int(name.split('topic_coords')[1].split(':')[0])\n",
    "            new_posx = pos[node_idx][0]\n",
    "            new_posy = pos[node_idx][1]\n",
    "\n",
    "            new_arr = np.array([[new_posx, new_posy]], dtype=np.float32)\n",
    "            sess.run([name_tensors[name].assign(new_arr)])\n",
    "\n",
    "    #==========================\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=config.max_to_keep)\n",
    "    update_tree_flg = False\n",
    "\n",
    "    # train model\n",
    "    time_start = time.time()\n",
    "    time_start = time.time()\n",
    "    print('---START TRAINING----------')\n",
    "    train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "    dev_batches = get_batches(instances_train, config.batch_size)\n",
    "    while epoch < config.n_epochs:\n",
    "        losses_train = []\n",
    "        ppls_train = []\n",
    "        for ct, batch in enumerate(train_batches):\n",
    "            feed_dict, current_batch_size = model.get_feed_dict(batch, epoch)\n",
    "\n",
    "            if current_batch_size!=config.batch_size:\n",
    "                continue\n",
    "\n",
    "            _, loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, ppls_batch, \\\n",
    "                                                    tree_prob_leaf, prob_depth, topic_bow, gamma_topic, topic_coords, reg3, prob_topic,\\\n",
    "                                                    grad_vars, logvars_bow,\\\n",
    "                                                                                                            global_step_log = \\\n",
    "            sess.run([model.opt, model.loss, model.topic_loss_recon, model.topic_loss_kl, model.topic_loss_reg, model.topic_ppls, \\\n",
    "                                           model.tree_prob_leaf, model.prob_depth, model.topic_bow, model.gamma_topic, model.topic_coords,model.coord_reg3, model.prob_topic,\\\n",
    "                                            model.grad_vars, model.logvars_bow,\\\n",
    "                                                                                                            tf.train.get_global_step()], feed_dict = feed_dict)\n",
    "\n",
    "\n",
    "            losses_train += [[loss_batch, topic_loss_recon_batch, topic_loss_kl_batch, topic_loss_reg_batch, reg3]]\n",
    "\n",
    "            ppls_train += list(ppls_batch)\n",
    "\n",
    "        if (epoch % config.log_period==0) and (epoch>0) and (epoch!=config.n_epochs):#global_step_log % config.log_period == 0:#config.log_period\n",
    "            print('epoch: ', epoch)\n",
    "        # if ct%10==0:\n",
    "            # validate\n",
    "            loss_train, topic_loss_recon_train, topic_loss_kl_train, topic_loss_reg_train, reg3 = np.mean(losses_train, 0)#losses_train#np.mean(losses_train, 0)\n",
    "            # print(loss_train)\n",
    "            ppl_train = np.exp(np.mean(ppls_train))\n",
    "            loss_dev, topic_loss_recon_dev, topic_loss_kl_dev, topic_loss_reg_dev, ppl_dev, probs_topic_dev = validate(sess, dev_batches, epoch, model, config)\n",
    "\n",
    "            # visualize topic\n",
    "            topics_freq_indices = np.argsort(sess.run(model.topic_bow), 1)[:, ::-1][:, :config.n_freq]\n",
    "            topics_freq_idxs = bow_idxs[topics_freq_indices]\n",
    "            topic_freq_tokens = {topic_idx: [idx_to_word[idx] for idx in topic_freq_idxs] for topic_idx, topic_freq_idxs in zip(model.topic_idxs, topics_freq_idxs)}\n",
    "            topic_prob_topic = {topic_idx: prob_topic for topic_idx, prob_topic in zip(model.topic_idxs, probs_topic_dev)}\n",
    "            descendant_idxs = {parent_idx: get_descendant_idxs(model, parent_idx) for parent_idx in model.topic_idxs}\n",
    "            recur_prob_topic = {parent_idx: np.sum([topic_prob_topic[child_idx] for child_idx in recur_child_idxs]) for parent_idx, recur_child_idxs in descendant_idxs.items()}\n",
    "\n",
    "            depth_specs = compute_topic_specialization(sess, model, instances_train)\n",
    "            hierarchical_affinities = compute_hierarchical_affinity(sess, model)\n",
    "\n",
    "            # save log\n",
    "            #### PRINT\n",
    "            time_log = int(time.time() - time_start)\n",
    "            log_series = pd.Series([time_log, epoch, ct, \\\n",
    "                    '%.2f'%loss_train, '%.0f'%ppl_train, '%.2f'%topic_loss_recon_train, '%.2f'%topic_loss_kl_train, '%.2f'%topic_loss_reg_train, \\\n",
    "                    '%.2f'%loss_dev, '%.0f'%ppl_dev, '%.2f'%topic_loss_recon_dev, '%.2f'%topic_loss_kl_dev, '%.2f'%topic_loss_reg_dev, \\\n",
    "                    '%.2f'%depth_specs[1], '%.2f'%depth_specs[2], '%.2f'%depth_specs[3], \\\n",
    "                    '%.2f'%hierarchical_affinities[0], '%.2f'%hierarchical_affinities[1]],\n",
    "                    index=log_df.columns)\n",
    "            log_df.loc[global_step_log] = log_series\n",
    "            print(log_series)\n",
    "            #### PRINT\n",
    "            #cPickle.dump(log_df, open(os.path.join(config.path_log), 'wb'))\n",
    "            print_topic_sample(sess, model, topic_prob_topic=topic_prob_topic, recur_prob_topic=recur_prob_topic, topic_freq_tokens=topic_freq_tokens)\n",
    "\n",
    "            # update tree\n",
    "            if not config.static:\n",
    "                if epoch>=2000:\n",
    "                    disable_add = True\n",
    "                else:\n",
    "                    disable_add = False\n",
    "                config.tree_idxs, update_tree_flg = model.update_tree(topic_prob_topic, recur_prob_topic, disable_add)\n",
    "                if update_tree_flg:\n",
    "\n",
    "                    prev_name_tensors = {tensor.name: variable for tensor, variable in zip(tf.global_variables(), sess.run(tf.global_variables()))} # store paremeters\n",
    "                    if 'sess' in globals(): sess.close()\n",
    "\n",
    "                    model = HierarchicalNeuralTopicModel(config)\n",
    "                    sess = tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1))\n",
    "                    name_tensors = {tensor.name: tensor for tensor in tf.global_variables()}\n",
    "                    #=== INIT NON INIT VARS=======\n",
    "                    non_init_coords = []\n",
    "                    docx_params = []\n",
    "                    for name in name_tensors.keys():\n",
    "    #                     if ('topic_coords' in name) and (name not in prev_name_tensors.keys()):\n",
    "    #                         non_init_coords.append(name)\n",
    "                        if ('_docx' in name):\n",
    "                            docx_params.append(name)\n",
    "    #                 sess.run([name_tensors[name].assign(variable) for name, variable in prev_name_tensors.items()]) # restore parameters\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "#                     sess.run([name_tensors[name].assign(variable) for name, variable in prev_name_tensors.items()  if name in name_tensors.keys() and (name not in docx_params)])#and (name not in docx_params)\n",
    "                    sess.run([name_tensors[name].assign(variable) for name, variable in prev_name_tensors.items()  if name in name_tensors.keys()])#and (name not in docx_params)\n",
    "\n",
    "                    max_coord = sess.run(tf.reduce_max(tf.math.abs(tf.convert_to_tensor(list(model.topic_coords.values())))))\n",
    "\n",
    "                    topic_coords = sess.run(model.topic_coords)\n",
    "\n",
    "                    highest_p_dict = {}\n",
    "\n",
    "                    for name in name_tensors.keys():\n",
    "                        if ('Adagrad' not in name) and ('topic_coords' in name) and (name not in prev_name_tensors.keys()):\n",
    "                            node_idx = int(name.split('topic_coords')[1].split(':')[0])\n",
    "                            node_level = model.node_level[node_idx]\n",
    "\n",
    "                            parent = model.child_to_parent_idxs[node_idx]\n",
    "                            highest_p = 0\n",
    "                            node_highest_p = parent\n",
    "\n",
    "                            node_x = topic_coords.get(node_highest_p)[:,0][0]#/100.\n",
    "                            node_y = topic_coords.get(node_highest_p)[:,1][0]#/100.\n",
    "\n",
    "                            new_arr = np.array([[node_x+random.uniform(0, 0.01), node_y + random.uniform(0, 0.01)]], dtype=np.float32)\n",
    "                            sess.run([name_tensors[name].assign(new_arr)])\n",
    "\n",
    "\n",
    "                    saver = tf.train.Saver(max_to_keep=1)\n",
    "                    gc.collect()\n",
    "            time_start = time.time()\n",
    "\n",
    "        train_batches = get_batches(instances_train, config.batch_size, iterator=True)\n",
    "        epoch += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tree structure: model.tree_idxs\n",
    "* Node level: model.level_nodes\n",
    "* Topic coordinates: model.topic_coords\n",
    "* Doc coordinates: model.latents_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
